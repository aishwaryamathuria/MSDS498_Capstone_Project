{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43944afc-f0f4-4145-810e-96f091b676ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "  <h2>RSNA Pneumonia Dataset - Model Selection</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988894e2-f670-457d-926f-e09062444433",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Import Libraries and Load Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea76c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain_v1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b94133",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"lxyuan/vit-xray-pneumonia-classification\"\n",
    "LABELS_CSV = \"/Users/tanmayswami/Downloads/stage_2_train_labels.csv\"\n",
    "IMAGES_DIR = \"/Users/tanmayswami/Downloads/stage_2_train_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55dba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POS = 500  \n",
    "N_NEG = 500  \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "SEED = 42\n",
    "\n",
    "PATIENT_ID_REGEX = re.compile(\n",
    "    r\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c02d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /opt/anaconda3/envs/langchain_v1/lib/python3.10/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b37ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def dicom_to_pil(path):\n",
    "    ds = pydicom.dcmread(path)\n",
    "    img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "    if getattr(ds, \"PhotometricInterpretation\", \"\") == \"MONOCHROME1\":\n",
    "        img = img.max() - img\n",
    "\n",
    "    img -= img.min()\n",
    "    if img.max() > 0:\n",
    "        img /= img.max()\n",
    "    img = (img * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    return Image.fromarray(img).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dac888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              patientId  \\\n",
      "0  7be6b4de-afe9-43c0-a581-0f49608c8976   \n",
      "1  2dcdd159-2889-48d3-a0ce-5c7b1086c49d   \n",
      "2  d8e66874-305e-4c80-9b75-5e764eb718ff   \n",
      "3  22f2d3ec-f7ea-4778-850d-bb111590202f   \n",
      "4  cdaa07d4-4234-4cd2-b9bf-abbf5aed1bb4   \n",
      "\n",
      "                                                path  \n",
      "0  /Users/tanmayswami/Downloads/stage_2_train_ima...  \n",
      "1  /Users/tanmayswami/Downloads/stage_2_train_ima...  \n",
      "2  /Users/tanmayswami/Downloads/stage_2_train_ima...  \n",
      "3  /Users/tanmayswami/Downloads/stage_2_train_ima...  \n",
      "4  /Users/tanmayswami/Downloads/stage_2_train_ima...  \n",
      "Indexed: 26684\n"
     ]
    }
   ],
   "source": [
    "def index_images(images_dir):\n",
    "    rows = []\n",
    "    p = Path(images_dir)\n",
    "\n",
    "    for fp in p.glob(\"*.dcm\"): \n",
    "        patient_id = fp.stem \n",
    "        rows.append({\"patientId\": patient_id, \"path\": str(fp)})\n",
    "\n",
    "    out = pd.DataFrame(rows).drop_duplicates(subset=[\"patientId\"])\n",
    "\n",
    "    if out.empty:\n",
    "        raise ValueError(f\"No .dcm files found in: {images_dir}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "img_df = index_images(IMAGES_DIR)\n",
    "print(img_df.head())\n",
    "print(\"Indexed:\", len(img_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4ba0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    20672\n",
       "1     6012\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_patient_labels(labels_csv):\n",
    "    df = pd.read_csv(labels_csv)\n",
    "    if \"Target\" not in df.columns or \"patientId\" not in df.columns:\n",
    "        raise ValueError(\"Expected columns: patientId, Target in RSNA labels CSV.\")\n",
    "    y = df.groupby(\"patientId\")[\"Target\"].max().reset_index()\n",
    "    y.rename(columns={\"Target\": \"y\"}, inplace=True)\n",
    "    return y\n",
    "\n",
    "build_patient_labels(LABELS_CSV)['y'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc410414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26684 26684 26684\n",
      "y\n",
      "0    20672\n",
      "1     6012\n",
      "Name: count, dtype: int64\n",
      "Pos %: 0.225303552690751\n"
     ]
    }
   ],
   "source": [
    "img_df = index_images(IMAGES_DIR)\n",
    "y_df = build_patient_labels(LABELS_CSV)\n",
    "df = img_df.merge(y_df, on=\"patientId\", how=\"inner\")\n",
    "print(len(img_df), len(y_df), len(df)) \n",
    "\n",
    "print(df[\"y\"].value_counts())\n",
    "print(\"Pos %:\", df[\"y\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359cce4-627b-4e9b-b08a-b31569fcc15c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Model 1: vit-xray-pneumonia-classification</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Model id2label: {0: 'NORMAL', 1: 'PNEUMONIA'}\n",
      "Matched images with labels: 26,684\n",
      "Eval set size: 1000 (pos=500, neg=500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 32/32 [00:24<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "AUROC: 0.8144\n",
      "Accuracy: 0.6770\n",
      "Precision: 0.6151 | Recall: 0.9460 | F1: 0.7455\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[204 296]\n",
      " [ 27 473]]\n",
      "Elapsed: 24.12s | Throughput: 41.47 images/sec | Batch size: 32\n",
      "Positive class index used: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def build_patient_labels(labels_csv):\n",
    "    # Defining patient-level y = max(Target) across rows.\n",
    "    df = pd.read_csv(labels_csv)\n",
    "    if \"Target\" not in df.columns or \"patientId\" not in df.columns:\n",
    "        raise ValueError(\"Expected columns: patientId, Target in RSNA labels CSV.\")\n",
    "    y = df.groupby(\"patientId\")[\"Target\"].max().reset_index()\n",
    "    y.rename(columns={\"Target\": \"y\"}, inplace=True)\n",
    "    return y\n",
    "\n",
    "def index_images(images_dir):\n",
    "    rows = []\n",
    "    p = Path(images_dir)\n",
    "\n",
    "    for fp in p.glob(\"*.dcm\"):\n",
    "        patient_id = fp.stem \n",
    "        rows.append({\"patientId\": patient_id, \"path\": str(fp)})\n",
    "\n",
    "    out = pd.DataFrame(rows).drop_duplicates(subset=[\"patientId\"])\n",
    "\n",
    "    if out.empty:\n",
    "        raise ValueError(f\"No .dcm files found in: {images_dir}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"path\"]\n",
    "        y = int(row[\"y\"])\n",
    "\n",
    "        if path.lower().endswith(\".dcm\"):\n",
    "            img = dicom_to_pil(path)\n",
    "        else:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return pixel_values, y\n",
    "\n",
    "def infer_logits(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_y = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pixel_values, y in tqdm(loader, desc=\"Inference\"):\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_y.append(y.numpy())\n",
    "\n",
    "    t1 = time.time()\n",
    "    probs = np.vstack(all_probs)\n",
    "    y_true = np.concatenate(all_y)\n",
    "\n",
    "    elapsed = t1 - t0\n",
    "    ips = len(y_true) / elapsed if elapsed > 0 else float(\"inf\")\n",
    "    return probs, y_true, elapsed, ips\n",
    "\n",
    "def main():\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(device)\n",
    "\n",
    "    id2label = getattr(model.config, \"id2label\", None)\n",
    "    print(\"Model id2label:\", id2label)\n",
    "\n",
    "    y_df = build_patient_labels(LABELS_CSV)\n",
    "    img_df = index_images(IMAGES_DIR)\n",
    "\n",
    "    df = img_df.merge(y_df, on=\"patientId\", how=\"inner\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No overlap between images and labels. Check your folder and CSV patientIds.\")\n",
    "    print(f\"Matched images with labels: {len(df):,}\")\n",
    "\n",
    "    pos = df[df[\"y\"] == 1]\n",
    "    neg = df[df[\"y\"] == 0]\n",
    "\n",
    "    if len(pos) == 0 or len(neg) == 0:\n",
    "        raise ValueError(f\"Need both classes. Found pos={len(pos)}, neg={len(neg)}\")\n",
    "\n",
    "    pos_s = pos.sample(n=min(N_POS, len(pos)), random_state=SEED)\n",
    "    neg_s = neg.sample(n=min(N_NEG, len(neg)), random_state=SEED)\n",
    "\n",
    "    eval_df = pd.concat([pos_s, neg_s]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(f\"Eval set size: {len(eval_df)} (pos={eval_df['y'].sum()}, neg={(eval_df['y']==0).sum()})\")\n",
    "\n",
    "    ds = RSNADataset(eval_df, processor)\n",
    "    NUM_WORKERS = 0\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    probs, y_true, elapsed, ips = infer_logits(model, loader, device)\n",
    "\n",
    "    if id2label and len(id2label) == probs.shape[1]:\n",
    "        labels = {int(k): v.lower() for k, v in id2label.items()}\n",
    "        pos_idx = None\n",
    "        for k, v in labels.items():\n",
    "            if \"pneum\" in v or \"opacity\" in v or \"lung\" in v:\n",
    "                pos_idx = k\n",
    "                break\n",
    "        if pos_idx is None:\n",
    "            pos_idx = 1 if probs.shape[1] > 1 else 0\n",
    "    else:\n",
    "        pos_idx = 1 if probs.shape[1] > 1 else 0\n",
    "\n",
    "    y_score = probs[:, pos_idx]\n",
    "    y_pred = (y_score >= 0.8).astype(int)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"AUROC: {auc:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix [[TN FP],[FN TP]]:\")\n",
    "    print(cm)\n",
    "    print(f\"Elapsed: {elapsed:.2f}s | Throughput: {ips:.2f} images/sec | Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Positive class index used: {pos_idx}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b9842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.25.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting torch==2.10.0 (from torchvision)\n",
      "  Downloading torch-2.10.0-1-cp310-none-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-12.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting filelock (from torch==2.10.0->torchvision)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.10.0->torchvision)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.10.0->torchvision)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch==2.10.0->torchvision)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.10.0->torchvision)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch==2.10.0->torchvision)\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.10.0->torchvision)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.10.0->torchvision)\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading torchvision-0.25.0-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.10.0-1-cp310-none-macosx_11_0_arm64.whl (79.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.1.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
      "\u001b[2K  Attempting uninstall: mpmath\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [sympy]ensions]\n",
      "\u001b[2K  Attempting uninstall: pillow90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: pillow 11.3.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling pillow-11.3.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled pillow-11.3.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: networkx[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: networkx 3.4.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling networkx-3.4.2:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.4.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.3━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.3:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: fsspec0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.10.0━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling fsspec-2025.10.0:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.10.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: filelockm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: filelock 3.20.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling filelock-3.20.0:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.20.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: jinja290m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.6━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.6━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: torch[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: torch 2.10.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling torch-2.10.0:━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m10/12\u001b[0m [torch]]\n",
      "\u001b[2K      Successfully uninstalled torch-2.10.090m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m10/12\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.4.1 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.2.0 which is incompatible.\n",
      "gradio 5.49.1 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.3 fsspec-2026.2.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pillow-12.1.0 sympy-1.14.0 torch-2.10.0 torchvision-0.25.0 typing-extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-cache-dir --force-reinstall torchvision -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4ba064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0 0.25.0\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c533df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a88629-ab9a-49cf-99f6-cfcd30f3b3fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Model 2: Densenet with transfer learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Total matched: 26684\n",
      "y\n",
      "0    20672\n",
      "1     6012\n",
      "Name: count, dtype: int64 | Pos %: 0.225303552690751\n",
      "Train: 4000 Eval: 1000\n",
      "Train pos: 2000 Eval pos: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8\n",
      "Train loss: 0.5089\n",
      "Eval AUROC: 0.8658 | Acc: 0.6500\n",
      "Precision: 0.9076 | Recall: 0.3340 | F1: 0.4883\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[483  17]\n",
      " [333 167]]\n",
      "Elapsed: 12.36s | Throughput: 80.88 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/8\n",
      "Train loss: 0.4152\n",
      "Eval AUROC: 0.8799 | Acc: 0.7270\n",
      "Precision: 0.9097 | Recall: 0.5040 | F1: 0.6486\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[475  25]\n",
      " [248 252]]\n",
      "Elapsed: 12.22s | Throughput: 81.84 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/8\n",
      "Train loss: 0.3461\n",
      "Eval AUROC: 0.8871 | Acc: 0.7560\n",
      "Precision: 0.9129 | Recall: 0.5660 | F1: 0.6988\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[473  27]\n",
      " [217 283]]\n",
      "Elapsed: 12.22s | Throughput: 81.84 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/8\n",
      "Train loss: 0.2779\n",
      "Eval AUROC: 0.8840 | Acc: 0.7560\n",
      "Precision: 0.9076 | Recall: 0.5700 | F1: 0.7002\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[471  29]\n",
      " [215 285]]\n",
      "Elapsed: 12.10s | Throughput: 82.63 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/8\n",
      "Train loss: 0.1982\n",
      "Eval AUROC: 0.8894 | Acc: 0.7940\n",
      "Precision: 0.8326 | Recall: 0.7360 | F1: 0.7813\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[426  74]\n",
      " [132 368]]\n",
      "Elapsed: 12.10s | Throughput: 82.63 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/8\n",
      "Train loss: 0.1336\n",
      "Eval AUROC: 0.8904 | Acc: 0.7820\n",
      "Precision: 0.8917 | Recall: 0.6420 | F1: 0.7465\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[461  39]\n",
      " [179 321]]\n",
      "Elapsed: 12.47s | Throughput: 80.18 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/8\n",
      "Train loss: 0.1326\n",
      "Eval AUROC: 0.8733 | Acc: 0.7940\n",
      "Precision: 0.8063 | Recall: 0.7740 | F1: 0.7898\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[407  93]\n",
      " [113 387]]\n",
      "Elapsed: 12.27s | Throughput: 81.50 imgs/s | Batch: 32 | Thresh: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/8\n",
      "Train loss: 0.1072\n",
      "Eval AUROC: 0.8806 | Acc: 0.8130\n",
      "Precision: 0.8111 | Recall: 0.8160 | F1: 0.8136\n",
      "Confusion Matrix [[TN FP],[FN TP]]:\n",
      "[[405  95]\n",
      " [ 92 408]]\n",
      "Elapsed: 12.23s | Throughput: 81.73 imgs/s | Batch: 32 | Thresh: 0.8\n",
      "\n",
      "Saved DenseNet checkpoint to: rsna_densenet121_epoch_last.pt\n",
      "\n",
      "Classification report (DenseNet @ thresh=0.8):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.81      0.81      0.81       500\n",
      "   PNEUMONIA       0.81      0.82      0.81       500\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.81      0.81      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LABELS_CSV = \"/Users/tanmayswami/Downloads/stage_2_train_labels.csv\"\n",
    "IMAGES_DIR = \"/Users/tanmayswami/Downloads/stage_2_train_images\"\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0       \n",
    "LR = 1e-4\n",
    "EPOCHS = 8            \n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "N_POS_EVAL = 500\n",
    "N_NEG_EVAL = 500\n",
    "\n",
    "N_POS_TRAIN = 2000\n",
    "N_NEG_TRAIN = 2000\n",
    "\n",
    "THRESH = 0.8\n",
    "\n",
    "DENSENET_SAVE_PATH = \"rsna_densenet121_epoch_last.pt\"\n",
    "VIT_SAVE_PATH = \"rsna_vit_hf_state_dict.pt\" \n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "class RSNADatasetTorchvision(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tfm: transforms.Compose):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tfm = tfm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = dicom_to_pil(row[\"path\"]) \n",
    "        y = int(row[\"y\"])\n",
    "        x = self.tfm(img)            \n",
    "        return x, y\n",
    "\n",
    "def build_rsna_df(labels_csv, images_dir):\n",
    "    y_df = build_patient_labels(labels_csv)   \n",
    "    img_df = index_images(images_dir)        \n",
    "    df = img_df.merge(y_df, on=\"patientId\", how=\"inner\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No overlap between images and labels. Check paths.\")\n",
    "    return df\n",
    "\n",
    "df_all = build_rsna_df(LABELS_CSV, IMAGES_DIR)\n",
    "print(\"Total matched:\", len(df_all))\n",
    "print(df_all[\"y\"].value_counts(), \"| Pos %:\", df_all[\"y\"].mean())\n",
    "\n",
    "def make_balanced_sample(df, n_pos, n_neg, seed):\n",
    "    pos = df[df[\"y\"] == 1]\n",
    "    neg = df[df[\"y\"] == 0]\n",
    "    if len(pos) == 0 or len(neg) == 0:\n",
    "        raise ValueError(\"Need both classes.\")\n",
    "    pos_s = pos.sample(n=min(n_pos, len(pos)), random_state=seed)\n",
    "    neg_s = neg.sample(n=min(n_neg, len(neg)), random_state=seed)\n",
    "    out = pd.concat([pos_s, neg_s]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "train_df = make_balanced_sample(df_all, N_POS_TRAIN, N_NEG_TRAIN, SEED)\n",
    "eval_df  = make_balanced_sample(df_all, N_POS_EVAL,  N_NEG_EVAL,  SEED + 1)\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Eval:\", len(eval_df))\n",
    "print(\"Train pos:\", train_df[\"y\"].sum(), \"Eval pos:\", eval_df[\"y\"].sum())\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "eval_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_ds = RSNADatasetTorchvision(train_df, train_tfm)\n",
    "eval_ds  = RSNADatasetTorchvision(eval_df,  eval_tfm)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "eval_loader  = DataLoader(eval_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "def build_densenet121(num_classes=2):\n",
    "    m = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "    in_features = m.classifier.in_features\n",
    "    m.classifier = nn.Linear(in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "model = build_densenet121(num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for x, y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "    return total_loss / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_y = []\n",
    "    t0 = time.time()\n",
    "    for x, y in tqdm(loader, desc=\"Infer\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_y.append(y.numpy())\n",
    "    t1 = time.time()\n",
    "    probs = np.vstack(all_probs)\n",
    "    y_true = np.concatenate(all_y)\n",
    "    elapsed = t1 - t0\n",
    "    ips = len(y_true) / elapsed if elapsed > 0 else float(\"inf\")\n",
    "    return probs, y_true, elapsed, ips\n",
    "\n",
    "def evaluate_binary_from_probs(probs, y_true, pos_idx, thresh):\n",
    "    y_score = probs[:, pos_idx]\n",
    "    y_pred = (y_score >= thresh).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_score) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"auc\": auc, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "        \"cm\": cm, \"y_score\": y_score, \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    probs, y_true, elapsed, ips = infer_probs(model, eval_loader, device)\n",
    "    metrics = evaluate_binary_from_probs(probs, y_true, pos_idx=1, thresh=THRESH)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"Train loss: {loss:.4f}\")\n",
    "    print(f\"Eval AUROC: {metrics['auc']:.4f} | Acc: {metrics['acc']:.4f}\")\n",
    "    print(f\"Precision: {metrics['prec']:.4f} | Recall: {metrics['rec']:.4f} | F1: {metrics['f1']:.4f}\")\n",
    "    print(\"Confusion Matrix [[TN FP],[FN TP]]:\")\n",
    "    print(metrics[\"cm\"])\n",
    "    print(f\"Elapsed: {elapsed:.2f}s | Throughput: {ips:.2f} imgs/s | Batch: {BATCH_SIZE} | Thresh: {THRESH}\")\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_name\": \"densenet121\",\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"threshold\": THRESH,\n",
    "        \"config\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"lr\": LR,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"train_pos\": N_POS_TRAIN,\n",
    "            \"train_neg\": N_NEG_TRAIN,\n",
    "            \"eval_pos\": N_POS_EVAL,\n",
    "            \"eval_neg\": N_NEG_EVAL,\n",
    "            \"seed\": SEED,\n",
    "        },\n",
    "    },\n",
    "    DENSENET_SAVE_PATH\n",
    ")\n",
    "print(f\"\\nSaved DenseNet checkpoint to: {DENSENET_SAVE_PATH}\")\n",
    "\n",
    "def load_densenet_checkpoint(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    m = build_densenet121(num_classes=2).to(device)\n",
    "    m.load_state_dict(ckpt[\"state_dict\"])\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "loaded_model = load_densenet_checkpoint(DENSENET_SAVE_PATH, device)\n",
    "print(\"\\nClassification report (DenseNet @ thresh={}):\".format(THRESH))\n",
    "print(classification_report(y_true, metrics[\"y_pred\"], target_names=[\"NORMAL\", \"PNEUMONIA\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09473aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
